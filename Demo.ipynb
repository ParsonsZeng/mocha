{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Implementation of Monotonic Chunkwise Attention (MoChA)\n",
    "\n",
    "This notebook shows how to compute the probability distribution for Monotonic Chunkwise Attention (MoChA) [1] efficiently using TensorFlow.  We give a brief overview of MoChA below; a more thorough treatment is in [1].\n",
    "\n",
    "We consider attention mechanisms, which given a length-$T$ memory $h_1, h_2, \\ldots, h_T$ produce an attention distribution $\\alpha_{i, j}$ and a \"context vector\" $c_i$ as\n",
    "\\begin{equation}\n",
    "c_i = \\sum_{j = 1}^T \\alpha_{i, j} h_j\n",
    "\\end{equation}\n",
    "In sequence-to-sequence settings, a context vector is computed for each output timestep $i$.\n",
    "In hard monotonic attention [2], for each $i$ there is a single index $t_i$ for which $\\alpha_{i, t_i} = 1$, and $\\alpha_{i, j} = 0$ otherwise.\n",
    "Further, the attention is constrained so that if $\\alpha_{i, t_i} = 1$ then $\\alpha_{i + 1, j} = 0$ for $j < t_i$.\n",
    "The result is that $c_i$ is effectively chosen to be $h_{t_i}$, and that once $c_i = h_{t_i}$, then none of $h_1, \\ldots, h_{k - 1}$ are chosen at subsequent output timesteps.\n",
    "Since this hard-assignment has zero derivative everywhere, a \"soft\" version is used during training, so that $\\alpha_{i, j}$ forms a probability distribution which obeys the above constraints in the limit of $\\alpha_{i, j}$ being all 0 or 1.\n",
    "This distribution can be computed efficiently in Tensorflow using the `tf.contrib.seq2seq.monotonic_attention` function.\n",
    "\n",
    "Monotonic Chunkwise Attention (MoChA) extends this so that $c_i$ is set to a weighted average of the $w$ memory entries before $t_i$, as chosen by a separate hard monotonic attention mechanism.\n",
    "Specifically, MoChA computes\n",
    "\\begin{align}\n",
    "v &= t_i - w + 1\\\\\n",
    "c_i &= \\sum_{k = v}^{t_i} \\frac{\\exp(u_{i, k})}{\\sum_{l = v}^{t_i} \\exp(u_{i, l})} h_k\n",
    "\\end{align}\n",
    "Note that we are effectively computing a softmax over the length-$w$ chunk, with logits $u_{i, j}$.\n",
    "During training, we use the induced probability distribution\n",
    "\\begin{align}\n",
    "\\beta_{i, j} &= \\sum_{k = j}^{j + w - 1} \\left( \\alpha_{i, k}\\exp(u_{i, j}) \\Bigg/\\sum_{l = k - w + 1}^k \\exp(u_{i, l}) \\right)\\\\\n",
    "c_i &= \\sum_{j = 1}^T \\beta_{i, j} h_j\n",
    "\\end{align}\n",
    "where $\\alpha_{i, j}$ is the soft probability distribution induced by hard monotonic attention.\n",
    "$\\beta_{i, :}$ can be computed efficiently in parallel by defining \n",
    "\\begin{equation}\n",
    "\\mathrm{MovingSum}(\\textbf{x}, b, f)_n := \\sum_{m = n - (b - 1)}^{n + f - 1} x_m\n",
    "\\end{equation}\n",
    "so that \n",
    "\\begin{equation}\n",
    "\\beta_{i, :} = \\exp(u_{i, :})\\,\\mathrm{MovingSum}\\left(\\frac{\\alpha_{i, :}}{\\mathrm{MovingSum}(\\exp(u_{i, :}), w, 1)}, 1, w \\right)\n",
    "\\end{equation}\n",
    "Note that in order to compute the softmax over the chunk in a numerically stable way, we need to ensure that the range of the logits $u_{i, j}$ is not large.\n",
    "One simple way to do this is to clip their range, which we demonstrate below.\n",
    "After that, we'll also demonstrate a way to do this exactly and stably using $Tw$ memory.\n",
    "\n",
    "[1] Chung-Cheng Chiu\\* and Colin Raffel\\*. \"*Monotonic Chunkwise Attention*\", in ICLR 2018.  \n",
    "[2] Colin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J. Weiss, and Douglas Eck. \"*Online and Linear-Time Attention by Enforcing Monotonic Alignments*\", in ICML 2017.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_sum(x, back, forward):\n",
    "    \"\"\"Compute the moving sum of x over a window with the provided bounds.\n",
    "\n",
    "    x is expected to be of shape (batch_size, sequence_length).\n",
    "    The returned tensor x_sum is computed as\n",
    "    x_sum[i, j] = x[i, j - back] + ... + x[i, j + forward]\n",
    "    \"\"\"\n",
    "    # Moving sum is computed as a carefully-padded 1D convolution with ones\n",
    "    x_padded = tf.pad(x, [[0, 0], [back, forward]])\n",
    "    # Add a \"channel\" dimension\n",
    "    x_padded = tf.expand_dims(x_padded, -1)\n",
    "    # Construct filters\n",
    "    filters = tf.ones((back + forward + 1, 1, 1))\n",
    "    x_sum = tf.nn.conv1d(x_padded, filters, 1, padding='VALID')\n",
    "    # Remove channel dimension\n",
    "    return x_sum[..., 0]\n",
    "\n",
    "def efficient_chunkwise_attention(chunk_size, emit_probs, softmax_logits):\n",
    "    \"\"\"Compute chunkwise attention distribution efficiently by clipping logits.\"\"\"\n",
    "    # Shift logits to avoid overflow\n",
    "    softmax_logits -= tf.reduce_max(softmax_logits, 1, keepdims=True)\n",
    "    # Limit the range for numerical stability\n",
    "    softmax_exp = tf.exp(softmax_logits)\n",
    "    softmax_exp = tf.maximum(softmax_exp, 1e-5)\n",
    "    # Compute chunkwise softmax denominators\n",
    "    softmax_denominators = moving_sum(softmax_exp, chunk_size - 1, 0)\n",
    "    # Compute \\beta_{i, :}. emit_probs are \\alpha_{i, :}.\n",
    "    probs = softmax_exp * moving_sum(emit_probs / softmax_denominators, 0, chunk_size - 1)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stable version\n",
    "\n",
    "Ideally, we'd like to compute this distribution stably and exactly without clipping the logits.  In order to do so, the softmax in each of the summation terms needs to be normalized, or in other words, the maximum of the logits within each chunk should be subtracted.  So, what we actually want to compute is\n",
    "$$\n",
    "\\beta_{i, j} = \\sum_{k = j}^{j + w - 1} \\left(\\alpha_{i, k} \\exp(u_{i, j} - m_{i, k}) \\Bigg / \\sum_{l = k - w + 1}^k \\exp(u_{i, l} - m_{i, k}) \\right)\n",
    "$$\n",
    "where $m_{i, k} = \\max(u_{i, k - w + 1}, \\ldots, u_{i, k})$.\n",
    "\n",
    "We can achieve this efficiently (i.e. completely in parallel), but we must use $Tw$ memory, as follows:\n",
    "1. Compute $m_{i, k}$ via max-pooling.\n",
    "1. Construct $T \\times w$ matrix $D$ where column $k$ is $[u_{i, k - w + 1}, \\ldots, u_{i, k}]$\n",
    "1. Subtract $m_{i, k}$ from $D$ (using broadcasting)\n",
    "1. Sum $\\exp(D)$ across columns to get $d$\n",
    "1. Construct $T \\times w$ matrix $E$ where column $k$ is $[d_{i, k}, \\ldots, d_{i, k + w - 1}]$\n",
    "1. Construct $T \\times w$ matrix $N$ where column $j$ is $[u_{i, j}, u_{i, j}, \\ldots, u_{i, j}]$\n",
    "1. Construct $T \\times w$ matrix $M$ where column $k$ is $[m_{i, k}, \\ldots, m_{i, k + w - 1}]$\n",
    "1. Subtract $M$ from $N$\n",
    "1. Compute $N = \\exp(N)$\n",
    "1. Construct $T \\times w$ matrix $A$ where column $j$ is $[\\alpha_{i, j}, \\ldots, \\alpha_{i, j + w - 1}]$\n",
    "1. Compute $AN/E$ (using broadcasting) and sum across columns to get the MoChA probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_max(x, w):\n",
    "    \"\"\"Compute the moving sum of x over a window with the provided bounds.\n",
    "\n",
    "    x is expected to be of shape (batch_size, sequence_length).\n",
    "    The returned tensor x_max is computed as\n",
    "    x_max[i, j] = max(x[i, j - window + 1], ..., x[i, j])\n",
    "    \"\"\"\n",
    "    # Pad x with -inf at the start\n",
    "    x = tf.pad(x, [[0, 0], [w - 1, 0]], mode='CONSTANT', constant_values=-np.inf)\n",
    "    # Add \"height\" and \"channel\" dimensions (max_pool operates on 2D)\n",
    "    x = tf.reshape(x, [tf.shape(x)[0], 1, tf.shape(x)[1], 1])\n",
    "    x = tf.nn.max_pool(x, [1, 1, w, 1], [1, 1, 1, 1], 'VALID')\n",
    "    # Remove \"height\" and \"channel\" dimensions\n",
    "    return x[:, 0, :, 0]\n",
    "\n",
    "def stable_chunkwise_attention(chunk_size, emit_probs, softmax_logits):\n",
    "    \"\"\"Compute chunkwise attention distriobution stably by subtracting logit max.\"\"\"\n",
    "    # Compute length-chunk_size sliding max of sequences in softmax_logits (m)\n",
    "    logits_max = moving_max(softmax_logits, chunk_size)\n",
    "\n",
    "    # Produce matrix with length-chunk_size frames of softmax_logits (D)\n",
    "    # Padding makes it so that the first frame is [-inf, -inf, ..., logits[0]]\n",
    "    padded_logits = tf.pad(softmax_logits, [[0, 0], [chunk_size - 1, 0]],\n",
    "                           constant_values=-np.inf)\n",
    "    framed_logits = tf.contrib.signal.frame(padded_logits, chunk_size, 1)\n",
    "    # Normalize each logit subsequence by the max in that subsequence\n",
    "    framed_logits = framed_logits - tf.expand_dims(logits_max, -1)\n",
    "    # Compute softmax denominators (d)\n",
    "    softmax_denominators = tf.reduce_sum(tf.exp(framed_logits), 2)\n",
    "    # Construct matrix of framed denominators, padding at the end so the final\n",
    "    # frame is [softmax_denominators[-1], inf, inf, ..., inf] (E)\n",
    "    framed_denominators = tf.contrib.signal.frame(\n",
    "        softmax_denominators, chunk_size, 1, pad_end=True, pad_value=np.inf)\n",
    "\n",
    "    # Create matrix of copied logits so that column j is softmax_logits[j] copied\n",
    "    # chunk_size times (N)\n",
    "    batch_size, seq_len = tf.unstack(tf.shape(softmax_logits))\n",
    "    copied_shape = (batch_size, seq_len, chunk_size)\n",
    "    copied_logits = (tf.expand_dims(softmax_logits, -1) *\n",
    "                     tf.ones(copied_shape, softmax_logits.dtype))\n",
    "    # Subtract the max over subsequences(M) from each logit\n",
    "    framed_max = tf.contrib.signal.frame(logits_max, chunk_size, 1,\n",
    "                                         pad_end=True, pad_value=np.inf)\n",
    "    copied_logits = copied_logits - framed_max\n",
    "    # Take exp() to get softmax numerators\n",
    "    softmax_numerators = tf.exp(copied_logits)\n",
    "\n",
    "    # Create matrix with length-chunk_size frames of emit_probs, padded so that\n",
    "    # the last frame is [emit_probs[-1], 0, 0, ..., 0] (A)\n",
    "    framed_probs = tf.contrib.signal.frame(emit_probs, chunk_size, 1, pad_end=True)\n",
    "  \n",
    "    # Compute chunkwise probability distributions\n",
    "    return tf.reduce_sum(framed_probs*softmax_numerators/framed_denominators, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we expect `efficient_chunkwise_attention` and `stable_chunkwise_attention` to be equivalent when the range of $u_{i, :}$ is relatively small.  When the difference between the smallest and largest $u_{i, :}$ is large, however, the \"efficient\" version will clip the logits and the \"stable\" version will produce the correct distribution.  We expect them to be about equally efficient, since they both are fully parallelizable, though the \"stable\" version takes about $w$ times more memory to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are efficient_chunkwise_attention and stable_chunkwise_attention the same with\n",
      "  small-magnitude softmax_logits? True\n",
      "  large-magnitude softmax_logits? False\n",
      "10000 loops, best of 10: 167 µs per loop\n",
      "10000 loops, best of 10: 164 µs per loop\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 50\n",
    "SEQUENCE_LENGTH = 100\n",
    "CHUNK_SIZE = 8\n",
    "\n",
    "g = tf.Graph()\n",
    "sess = tf.Session(graph=g)\n",
    "\n",
    "with g.as_default():\n",
    "    # Synthetic monotonic attention probabilities alpha_{i, j}\n",
    "    emit_probs_data = np.random.uniform(size=(BATCH_SIZE, SEQUENCE_LENGTH))\n",
    "    emit_probs_data /= np.sum(emit_probs_data, axis=1, keepdims=True)\n",
    "    # We'll use tf.Variables throughout for benchmarking reasons\n",
    "    emit_probs = tf.Variable(emit_probs_data.astype(np.float32))\n",
    "    # Synthetic softmax logits\n",
    "    softmax_logits_data = np.random.normal(size=(BATCH_SIZE, SEQUENCE_LENGTH))\n",
    "    softmax_logits = tf.Variable(softmax_logits_data.astype(np.float32))\n",
    "\n",
    "    # Test whether the efficient and stable versions compute the same thing\n",
    "    option_1 = efficient_chunkwise_attention(CHUNK_SIZE, emit_probs, softmax_logits)\n",
    "    option_2 = stable_chunkwise_attention(CHUNK_SIZE, emit_probs, softmax_logits)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    out_1, out_2 = sess.run([option_1, option_2])\n",
    "    print 'Are efficient_chunkwise_attention and stable_chunkwise_attention the same with'\n",
    "    print '  small-magnitude softmax_logits?', np.allclose(out_1, out_2)\n",
    "\n",
    "    # Test they no longer compute the same thing when the range of logits is large\n",
    "    softmax_logits_data[0, 5:7] -= 1e10\n",
    "    sess.run(softmax_logits.assign(softmax_logits_data))\n",
    "    out_1, out_2 = sess.run([option_1, option_2])\n",
    "    print '  large-magnitude softmax_logits?', np.allclose(out_1, out_2)\n",
    "\n",
    "    # Time them.  Use tf.group and pre-run to test graph execution time only.\n",
    "    option_1 = tf.group(option_1)\n",
    "    option_2 = tf.group(option_2)\n",
    "    sess.run(option_1)\n",
    "    sess.run(option_2)\n",
    "    %timeit -r10 sess.run(option_1)\n",
    "    %timeit -r10 sess.run(option_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
